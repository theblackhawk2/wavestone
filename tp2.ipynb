{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"credit_scoring.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seniority</th>\n",
       "      <th>Home</th>\n",
       "      <th>Time</th>\n",
       "      <th>Age</th>\n",
       "      <th>Marital</th>\n",
       "      <th>Records</th>\n",
       "      <th>Job</th>\n",
       "      <th>Expenses</th>\n",
       "      <th>Income</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Debt</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Price</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2985.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1468.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1577.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>915.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>4162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>16500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2201.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2189.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1159.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>1357.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2557.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1735.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>2072.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4355</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4357</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1704.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4359</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1342.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1656.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>975.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4370</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>963.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4375 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Seniority  Home  Time   Age  Marital  Records  Job  Expenses  Income  \\\n",
       "0           9.0   1.0  60.0  30.0      0.0      1.0  1.0      73.0   129.0   \n",
       "1          17.0   1.0  60.0  58.0      1.0      1.0  0.0      48.0   131.0   \n",
       "2          10.0   0.0  36.0  46.0      0.0      2.0  1.0      90.0   200.0   \n",
       "3           0.0   1.0  60.0  24.0      1.0      1.0  0.0      63.0   182.0   \n",
       "4           0.0   1.0  36.0  26.0      1.0      1.0  0.0      46.0   107.0   \n",
       "5           1.0   0.0  60.0  36.0      0.0      1.0  0.0      75.0   214.0   \n",
       "6          29.0   0.0  60.0  44.0      0.0      1.0  0.0      75.0   125.0   \n",
       "7           9.0   1.0  12.0  27.0      1.0      1.0  0.0      35.0    80.0   \n",
       "8           0.0   0.0  60.0  32.0      0.0      1.0  1.0      90.0   107.0   \n",
       "9           0.0   1.0  48.0  41.0      0.0      1.0  1.0      90.0    80.0   \n",
       "10          6.0   0.0  48.0  34.0      0.0      1.0  1.0      60.0   125.0   \n",
       "11          7.0   0.0  36.0  29.0      0.0      1.0  0.0      60.0   121.0   \n",
       "12          8.0   0.0  60.0  30.0      0.0      1.0  0.0      75.0   199.0   \n",
       "13         19.0   1.0  36.0  37.0      0.0      1.0  0.0      75.0   170.0   \n",
       "14          0.0   1.0  18.0  21.0      1.0      2.0  1.0      35.0    50.0   \n",
       "15          0.0   0.0  24.0  68.0      0.0      1.0  0.0      75.0   131.0   \n",
       "16         15.0   1.0  24.0  52.0      1.0      1.0  1.0      35.0   330.0   \n",
       "17         33.0   1.0  24.0  68.0      0.0      1.0  1.0      65.0   200.0   \n",
       "18          0.0   1.0  48.0  36.0      0.0      1.0  1.0      45.0   130.0   \n",
       "19          1.0   1.0  60.0  31.0      1.0      1.0  0.0      35.0   137.0   \n",
       "20          2.0   1.0  60.0  25.0      1.0      1.0  0.0      46.0   107.0   \n",
       "21          5.0   0.0  60.0  22.0      1.0      1.0  0.0      45.0   324.0   \n",
       "22          1.0   0.0  60.0  45.0      0.0      1.0  1.0     105.0   112.0   \n",
       "23         27.0   1.0  60.0  41.0      0.0      1.0  0.0      74.0   140.0   \n",
       "24         26.0   0.0  60.0  51.0      0.0      1.0  0.0      45.0   143.0   \n",
       "25         12.0   0.0  36.0  54.0      0.0      1.0  0.0      60.0   130.0   \n",
       "26         19.0   0.0  60.0  43.0      1.0      1.0  0.0      75.0   180.0   \n",
       "27         15.0   0.0  36.0  43.0      0.0      1.0  0.0      75.0   251.0   \n",
       "28          3.0   0.0  24.0  23.0      0.0      1.0  0.0      75.0    85.0   \n",
       "29          4.0   0.0  48.0  29.0      0.0      1.0  0.0      45.0   150.0   \n",
       "...         ...   ...   ...   ...      ...      ...  ...       ...     ...   \n",
       "4345        1.0   1.0  60.0  37.0      1.0      2.0  1.0      66.0    60.0   \n",
       "4346        5.0   0.0  60.0  31.0      0.0      1.0  0.0      35.0   100.0   \n",
       "4347        6.0   0.0  60.0  43.0      0.0      2.0  1.0      45.0   140.0   \n",
       "4348        1.0   0.0  36.0  26.0      0.0      1.0  0.0      35.0   154.0   \n",
       "4349       34.0   0.0  60.0  50.0      0.0      2.0  0.0      60.0   150.0   \n",
       "4350       20.0   0.0  48.0  47.0      0.0      1.0  0.0      75.0   175.0   \n",
       "4351        6.0   0.0  60.0  26.0      0.0      1.0  0.0      45.0   260.0   \n",
       "4352        4.0   1.0  36.0  38.0      0.0      1.0  1.0      75.0   170.0   \n",
       "4353        3.0   0.0  24.0  64.0      0.0      1.0  1.0      45.0   172.0   \n",
       "4354        0.0   1.0  60.0  36.0      0.0      2.0  1.0      75.0   150.0   \n",
       "4355        8.0   0.0  36.0  51.0      1.0      1.0  0.0      35.0    92.0   \n",
       "4356        6.0   1.0  60.0  31.0      0.0      1.0  0.0      45.0   199.0   \n",
       "4357        3.0   1.0  48.0  26.0      0.0      2.0  1.0      45.0     0.0   \n",
       "4358        1.0   1.0  48.0  18.0      1.0      1.0  1.0      35.0   130.0   \n",
       "4359        4.0   1.0  18.0  37.0      0.0      1.0  0.0      45.0   200.0   \n",
       "4360        8.0   0.0  60.0  37.0      0.0      1.0  0.0      75.0    50.0   \n",
       "4361       20.0   0.0  60.0  38.0      0.0      1.0  1.0      60.0     0.0   \n",
       "4362        3.0   0.0  42.0  41.0      0.0      1.0  1.0      35.0     0.0   \n",
       "4363       10.0   0.0  60.0  33.0      1.0      1.0  0.0      45.0   219.0   \n",
       "4364        2.0   0.0  60.0  31.0      1.0      1.0  0.0      35.0    63.0   \n",
       "4365        1.0   0.0  60.0  31.0      1.0      2.0  0.0      35.0   242.0   \n",
       "4366        6.0   1.0  60.0  22.0      1.0      2.0  0.0      35.0   100.0   \n",
       "4367        6.0   0.0  48.0  52.0      0.0      1.0  0.0      45.0   190.0   \n",
       "4368        3.0   0.0  60.0  49.0      0.0      1.0  0.0      35.0   160.0   \n",
       "4369        1.0   1.0  48.0  30.0      0.0      2.0  1.0      75.0    77.0   \n",
       "4370        1.0   1.0  60.0  39.0      0.0      1.0  0.0      69.0    92.0   \n",
       "4371       22.0   0.0  60.0  46.0      0.0      1.0  0.0      60.0    75.0   \n",
       "4372        0.0   0.0  24.0  37.0      0.0      1.0  1.0      60.0    90.0   \n",
       "4373        0.0   1.0  48.0  23.0      1.0      1.0  1.0      49.0   140.0   \n",
       "4374        5.0   0.0  60.0  32.0      0.0      1.0  1.0      60.0   140.0   \n",
       "\n",
       "       Assets    Debt  Amount   Price  Status  \n",
       "0         0.0     0.0   800.0   846.0       1  \n",
       "1         0.0     0.0  1000.0  1658.0       1  \n",
       "2      3000.0     0.0  2000.0  2985.0       0  \n",
       "3      2500.0     0.0   900.0  1325.0       1  \n",
       "4         0.0     0.0   310.0   910.0       1  \n",
       "5      3500.0     0.0   650.0  1645.0       1  \n",
       "6     10000.0     0.0  1600.0  1800.0       1  \n",
       "7         0.0     0.0   200.0  1093.0       1  \n",
       "8     15000.0     0.0  1200.0  1957.0       1  \n",
       "9         0.0     0.0  1200.0  1468.0       0  \n",
       "10     4000.0     0.0  1150.0  1577.0       1  \n",
       "11     3000.0     0.0   650.0   915.0       1  \n",
       "12     5000.0  2500.0  1500.0  1650.0       1  \n",
       "13     3500.0   260.0   600.0   940.0       1  \n",
       "14        0.0     0.0   400.0   500.0       0  \n",
       "15     4162.0     0.0   900.0  1186.0       1  \n",
       "16    16500.0     0.0  1500.0  2201.0       1  \n",
       "17     5000.0  2000.0   600.0  1350.0       1  \n",
       "18      750.0     0.0  1100.0  1511.0       0  \n",
       "19        0.0     0.0  1250.0  1253.0       1  \n",
       "20        0.0     0.0  1500.0  2189.0       0  \n",
       "21    10000.0     0.0  1100.0  1159.0       1  \n",
       "22     2000.0   500.0   600.0  1332.0       0  \n",
       "23        0.0     0.0   950.0  1497.0       1  \n",
       "24     3500.0     0.0  1350.0  1357.0       1  \n",
       "25     4000.0     0.0   700.0  2100.0       1  \n",
       "26     4000.0     0.0  1000.0  1070.0       1  \n",
       "27     4000.0     0.0  1800.0  2557.0       1  \n",
       "28     5000.0     0.0   600.0  1600.0       0  \n",
       "29     5000.0  3300.0  1100.0  1312.0       1  \n",
       "...       ...     ...     ...     ...     ...  \n",
       "4345      0.0     0.0  1500.0  2800.0       0  \n",
       "4346   5000.0     0.0   900.0  1263.0       1  \n",
       "4347   4300.0     0.0  2200.0  2500.0       1  \n",
       "4348   7000.0  1700.0  1400.0  1735.0       1  \n",
       "4349   9000.0     0.0  1300.0  1700.0       1  \n",
       "4350   4000.0     0.0   875.0  1098.0       1  \n",
       "4351   5000.0  2500.0  1300.0  2072.0       1  \n",
       "4352   5000.0  1500.0   650.0   700.0       0  \n",
       "4353   8000.0     0.0   400.0  1115.0       1  \n",
       "4354      0.0     0.0  1500.0  1642.0       0  \n",
       "4355   2500.0     0.0   875.0  1150.0       0  \n",
       "4356      0.0     0.0  1200.0  1700.0       1  \n",
       "4357      0.0     0.0  1400.0  1704.0       0  \n",
       "4358   2500.0   600.0  1280.0  1285.0       0  \n",
       "4359   3000.0     0.0   400.0  1800.0       1  \n",
       "4360  15000.0     0.0  1400.0  1645.0       0  \n",
       "4361  15000.0     0.0  1000.0  1638.0       1  \n",
       "4362  40000.0     0.0  1460.0  2191.0       1  \n",
       "4363   4000.0     0.0   950.0  1104.0       1  \n",
       "4364   2000.0     0.0  1000.0  1342.0       0  \n",
       "4365   9000.0     0.0  1500.0  1656.0       1  \n",
       "4366      0.0     0.0  1200.0  1496.0       0  \n",
       "4367   3500.0     0.0  1500.0  1905.0       1  \n",
       "4368   3000.0     0.0   900.0   975.0       1  \n",
       "4369      0.0     0.0  1200.0  1300.0       0  \n",
       "4370      0.0     0.0   900.0  1020.0       0  \n",
       "4371   3000.0   600.0   950.0  1263.0       1  \n",
       "4372   3500.0     0.0   500.0   963.0       0  \n",
       "4373      0.0     0.0   550.0   550.0       1  \n",
       "4374   4000.0  1000.0  1350.0  1650.0       1  \n",
       "\n",
       "[4375 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4375, 13)\n",
      "------------\n",
      "(4375,)\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "car = data.iloc[:,:-1].values\n",
    "pre = data.iloc[:,-1].values\n",
    "print(car.shape)\n",
    "print(\"------------\")\n",
    "print(pre.shape)\n",
    "print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1216.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "       3159.]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), <a list of 10 Patch objects>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(plt.hist(pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "car_train, car_test, pre_train, pre_test = train_test_split(car, pre, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 325  279]\n",
      " [ 318 1266]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.54      0.52       604\n",
      "           1       0.82      0.80      0.81      1584\n",
      "\n",
      "    accuracy                           0.73      2188\n",
      "   macro avg       0.66      0.67      0.67      2188\n",
      "weighted avg       0.73      0.73      0.73      2188\n",
      "\n",
      "Accuracy: 0.73\n",
      "\n",
      "\n",
      "[[ 189  415]\n",
      " [ 187 1397]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.31      0.39       604\n",
      "           1       0.77      0.88      0.82      1584\n",
      "\n",
      "    accuracy                           0.72      2188\n",
      "   macro avg       0.64      0.60      0.60      2188\n",
      "weighted avg       0.70      0.72      0.70      2188\n",
      "\n",
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_algo(car_train, car_test, pre_train, pre_test):\n",
    "    \n",
    "    #implantation arbre de décision \n",
    "    clf = DecisionTreeClassifier(random_state=1).fit(car_train,pre_train)\n",
    "    pre_pred = clf.predict(car_test)\n",
    "    \n",
    "    #Affichage de la matrice de confusion et des mesures de précision, rappel, f1-score et support\n",
    "    print(confusion_matrix(pre_test, pre_pred))\n",
    "    print(classification_report(pre_test, pre_pred))\n",
    "    #Affichage de la mesure de justesse\n",
    "    print('Accuracy: '+str(\"%0.2f\" % accuracy_score(pre_test, pre_pred)))\n",
    "    print('\\n')\n",
    "    #Implementation du classifieur KNN\n",
    "    nbrs = KNeighborsClassifier(n_neighbors=5).fit(car_train,pre_train)\n",
    "    pre_pred_knn = nbrs.predict(car_test) \n",
    "\n",
    "    print(confusion_matrix(pre_test, pre_pred_knn))  \n",
    "    print(classification_report(pre_test, pre_pred_knn)) \n",
    "    print('Accuracy: '+str(\"%0.2f\" % accuracy_score(pre_test, pre_pred_knn)))\n",
    "\n",
    "\n",
    "eval_algo(car_train,car_test,pre_train,pre_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 326  278]\n",
      " [ 322 1262]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.54      0.52       604\n",
      "           1       0.82      0.80      0.81      1584\n",
      "\n",
      "    accuracy                           0.73      2188\n",
      "   macro avg       0.66      0.67      0.66      2188\n",
      "weighted avg       0.73      0.73      0.73      2188\n",
      "\n",
      "Accuracy: 0.73\n",
      "\n",
      "\n",
      "[[ 282  322]\n",
      " [ 216 1368]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.47      0.51       604\n",
      "           1       0.81      0.86      0.84      1584\n",
      "\n",
      "    accuracy                           0.75      2188\n",
      "   macro avg       0.69      0.67      0.67      2188\n",
      "weighted avg       0.74      0.75      0.75      2188\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#Importation de StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fitting sur toute la base pour avoir la moyenne et la variance appliqué sur le total de la base de données\n",
    "scaler.fit(car)\n",
    "car_norm = scaler.transform(car)\n",
    "car_train_norm, car_test_norm = scaler.transform(car_train), scaler.transform(car_test)\n",
    "# Evaluation de l'algorithme avec les valeurs normalisés\n",
    "eval_algo(car_train_norm,car_test_norm,pre_train,pre_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ca soit pour le recall ou la précision, les performances se sont légèrement améliorés pour les 2 modèles, \n",
    "mais les conclusions restent les mêmes  \\\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 324  280]\n",
      " [ 330 1254]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.54      0.52       604\n",
      "           1       0.82      0.79      0.80      1584\n",
      "\n",
      "    accuracy                           0.72      2188\n",
      "   macro avg       0.66      0.66      0.66      2188\n",
      "weighted avg       0.73      0.72      0.72      2188\n",
      "\n",
      "Accuracy: 0.72\n",
      "\n",
      "\n",
      "[[ 286  318]\n",
      " [ 207 1377]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.47      0.52       604\n",
      "           1       0.81      0.87      0.84      1584\n",
      "\n",
      "    accuracy                           0.76      2188\n",
      "   macro avg       0.70      0.67      0.68      2188\n",
      "weighted avg       0.75      0.76      0.75      2188\n",
      "\n",
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=13)\n",
    "pca.fit(car_norm)         \n",
    "\n",
    "car_pca = pca.transform(car_norm)\n",
    "car_train_pca = pca.transform(car_train_norm)\n",
    "car_test_pca = pca.transform(car_test_norm)\n",
    "\n",
    "#On affiche la taille des composantes pca obtenues\n",
    "car_train_concat = np.concatenate((car_train_norm,car_train_pca[:,:3]), axis=1)\n",
    "car_test_concat  = np.concatenate((car_test_norm,car_test_pca[:,:3]), axis=1)\n",
    "\n",
    "eval_algo(car_train_concat,car_test_concat,pre_train,pre_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que les differences de performances sont vraiment très faibles,\n",
    "ceci était prévu puisque tous les prédicteurs qui ont été rajoutés sont des combinaisons linaires des autres et \\\\\n",
    "donc n'apportent pas de nouvelles informations qui vont améliorer ces performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Income', 'Seniority', 'Price', 'Amount', 'Age', 'Assets', 'Expenses',\n",
      "       'Records', 'Time', 'Job', 'Debt', 'Home', 'Marital'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(car_train_norm, pre_train)\n",
    "importances=clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],axis=0)\n",
    "\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "\n",
    "features = data.columns[:13]\n",
    "print(features[sorted_idx])\n",
    "\n",
    "padding = np.arange(car_train_norm.size/len(car_train_norm)) + 0.5 \n",
    "plt.barh(padding, importances[sorted_idx],xerr=std[sorted_idx], align='center') \n",
    "plt.yticks(padding, features[sorted_idx])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Variable Importance\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN=KNeighborsClassifier(n_neighbors=5)\n",
    "scores=np.zeros(car_train_norm.shape[1]+1)\n",
    "\n",
    "for f in np.arange(0, car_train_norm.shape[1]+1):\n",
    "  X1_f = car_train_norm[:,sorted_idx[:f+1]]\n",
    "  X2_f = car_test_norm[:,sorted_idx[:f+1]]\n",
    "  KNN.fit(X1_f,pre_train)\n",
    "  YKNN=KNN.predict(X2_f)\n",
    "  scores[f]=np.round(accuracy_score(pre_test,YKNN),3)\n",
    "plt.plot(scores)\n",
    "plt.xlabel(\"Nombre de Variables\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Evolution de l'accuracy en fonction des variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquon que l'accuracy evolue en fonction du nombre de variables, jusqu'a atteindre une valeur approximative\n",
    "de 75% pour un nombre de variables égale à 8, ensuite l'accuracy commence à decroitre puis finit par se stabiliser autour \n",
    "de 75%. On choisit donc 8 variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur classificateur knn :KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=50, p=2,\n",
      "                     weights='uniform')\n",
      "Meilleur classificateur Decision Tree :DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=900, splitter='random')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "params_knn = {\"n_neighbors\" : [1,3,5,10,20,50,100], \"metric\" : [\"minkowski\",\"euclidean\", \"chebyshev\"]}              \n",
    "clf_knn= KNeighborsClassifier()\n",
    "clf_knn= GridSearchCV(clf_knn,params_knn, scoring=scorer)\n",
    "clf_knn= clf_knn.fit(car_train_norm, pre_train)\n",
    "\n",
    "best_knn = clf_knn.best_estimator_\n",
    "\n",
    "print(\"Meilleur classificateur knn :\" +str(best_knn))\n",
    "\n",
    "params_decision = {'criterion':[\"gini\",\"entropy\"],\"splitter\":[\"best\",\"random\"],\"random_state\":[1,2,5,10,100,300,600,900,1000]}\n",
    "\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree = GridSearchCV(clf_tree, params_decision, scoring=scorer) \n",
    "clf_tree= clf_tree.fit(car_train_norm, pre_train)\n",
    "\n",
    "best_tree = clf_tree.best_estimator_\n",
    "\n",
    "print(\"Meilleur classificateur Decision Tree :\" +str(best_tree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs choisies sont k=50 et distance de 'minkowski' pour le modèle knn. Tandis que pour le modèle arbre de décision, le critère choisi est Gini avec un random_state de 900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import FeatureUnion \n",
    "\n",
    "pipeline = Pipeline(memory=None,steps=[\n",
    "    ('std', StandardScaler()),\n",
    "    ('ftu', FeatureUnion( [ ('std', StandardScaler()) , ('pca',PCA(0.75)) ] )), \n",
    "    ('clf', DecisionTreeClassifier())] )\n",
    "\n",
    "pipeline.fit(car_train, pre_train).predict(car_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pipline permet de minimiser le travail, en utilisant une seule ligne pour exécuter le programme, le résultat obtenu est ensuite utilisé pour faire une prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.000e+00 1.000e+00 6.000e+01 ... 0.000e+00 8.000e+02 8.460e+02]\n",
      " [1.700e+01 1.000e+00 6.000e+01 ... 0.000e+00 1.000e+03 1.658e+03]\n",
      " [1.000e+01 0.000e+00 3.600e+01 ... 0.000e+00 2.000e+03 2.985e+03]\n",
      " ...\n",
      " [0.000e+00 0.000e+00 2.400e+01 ... 0.000e+00 5.000e+02 9.630e+02]\n",
      " [0.000e+00 1.000e+00 4.800e+01 ... 0.000e+00 5.500e+02 5.500e+02]\n",
      " [5.000e+00 0.000e+00 6.000e+01 ... 1.000e+03 1.350e+03 1.650e+03]]\n",
      "[1 1 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans \n",
    "from sklearn.mixture import GaussianMixture \n",
    "from sklearn.tree import DecisionTreeClassifier #cart, id3 and decision stump (1 lvl tree)\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier #bagging, RF and adaboost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_score, accuracy_score, cohen_kappa_score, make_scorer\n",
    "import time\n",
    "\n",
    "#définir un dictionnaire dans lequel on met la liste des algorithmes à comparer\n",
    "\n",
    "clfs = {\n",
    "    #NaiveBayesSimple\n",
    "    'NBayes': GaussianNB(),\n",
    "    #Un arbre CART\n",
    "    'CART': tree.DecisionTreeClassifier(),\n",
    "    #Un arbre ID3\n",
    "    'ID3': DecisionTreeClassifier(criterion = \"entropy\"),\n",
    "    #MultilayerPerceptron à deux couches de tailles respectives 20 et 10\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(20,10),max_iter=1000),\n",
    "    #Random Forest avec 50 classifieurs\n",
    "    'RF': RandomForestClassifier(n_estimators=50),\n",
    "    #k-plus-proches-voisins avec k=5\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    #Bagging avec 50 classifieurs\n",
    "    'Bagging': BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50),\n",
    "    #AdaBoost avec 50 classifieurs \n",
    "    'Adaboost_depth1':AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50),\n",
    "    'Adaboost_depth2':AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=50)\n",
    "}\n",
    "\n",
    "acc_scorer=make_scorer(accuracy_score)\n",
    "auc_scorer=make_scorer(roc_auc_score)\n",
    "precision_scorer=make_scorer(precision_score)\n",
    "kappa_scorer=make_scorer(cohen_kappa_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_classifiers(clfs, X, Y):\n",
    "    result=dict()\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    for i in clfs:\n",
    "        init_time = time.time()\n",
    "        clf = clfs[i]\n",
    "        clf = clf.fit(X, Y)\n",
    "        cv_acc = cross_val_score(clf, X, Y, cv=kf)\n",
    "        kappa_scores=cross_val_score(estimator=clf,X=X,y=Y,cv=kf,scoring=kappa_scorer)\n",
    "        auc_scores= cross_val_score(estimator=clf,X=X,y=Y,cv=kf,scoring=auc_scorer)\n",
    "        algo_time = time.time() - init_time\n",
    "        print(\"Accuracy for {0} is: {1:.3f} +/- {2:.3f}\".format(i, np.mean(cv_acc), np.std(cv_acc)))\n",
    "        print(\"kappa for {0} is: {1:.3f} +/- {2:.3f}\".format(i, np.mean(kappa_scores), np.std(kappa_scores)))\n",
    "        print(\"AUC for {0} is: {1:.3f} +/- {2:.3f}\".format(i, np.mean(auc_scores), np.std(auc_scores)))        \n",
    "        print(\"execution time is: %s seconds\" % algo_time)\n",
    "        print('\\n')\n",
    "        result[i]=clf\n",
    "    return result    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for NBayes is: 0.772 +/- 0.022\n",
      "kappa for NBayes is: 0.412 +/- 0.053\n",
      "AUC for NBayes is: 0.699 +/- 0.024\n",
      "execution time is: 0.08808326721191406 seconds\n",
      "\n",
      "\n",
      "Accuracy for CART is: 0.728 +/- 0.026\n",
      "kappa for CART is: 0.311 +/- 0.067\n",
      "AUC for CART is: 0.657 +/- 0.027\n",
      "execution time is: 0.5718128681182861 seconds\n",
      "\n",
      "\n",
      "Accuracy for ID3 is: 0.723 +/- 0.022\n",
      "kappa for ID3 is: 0.304 +/- 0.049\n",
      "AUC for ID3 is: 0.655 +/- 0.026\n",
      "execution time is: 0.7810103893280029 seconds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MLP is: 0.738 +/- 0.047\n",
      "kappa for MLP is: 0.232 +/- 0.099\n",
      "AUC for MLP is: 0.639 +/- 0.050\n",
      "execution time is: 41.87214970588684 seconds\n",
      "\n",
      "\n",
      "Accuracy for RF is: 0.783 +/- 0.012\n",
      "kappa for RF is: 0.419 +/- 0.047\n",
      "AUC for RF is: 0.689 +/- 0.020\n",
      "execution time is: 6.350930213928223 seconds\n",
      "\n",
      "\n",
      "Accuracy for KNN is: 0.715 +/- 0.017\n",
      "kappa for KNN is: 0.204 +/- 0.043\n",
      "AUC for KNN is: 0.591 +/- 0.020\n",
      "execution time is: 0.7287325859069824 seconds\n",
      "\n",
      "\n",
      "Accuracy for Bagging is: 0.733 +/- 0.022\n",
      "kappa for Bagging is: 0.059 +/- 0.074\n",
      "AUC for Bagging is: 0.519 +/- 0.026\n",
      "execution time is: 3.127549171447754 seconds\n",
      "\n",
      "\n",
      "Accuracy for Adaboost_depth1 is: 0.788 +/- 0.016\n",
      "kappa for Adaboost_depth1 is: 0.428 +/- 0.044\n",
      "AUC for Adaboost_depth1 is: 0.698 +/- 0.021\n",
      "execution time is: 5.013266563415527 seconds\n",
      "\n",
      "\n",
      "Accuracy for Adaboost_depth2 is: 0.778 +/- 0.013\n",
      "kappa for Adaboost_depth2 is: 0.412 +/- 0.035\n",
      "AUC for Adaboost_depth2 is: 0.695 +/- 0.018\n",
      "execution time is: 6.98376727104187 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers=run_classifiers(clfs, car, pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Apprentissage supervisé : Données hétérogènes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.250</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00202</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.040</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00043</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.500</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.750</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00100</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.710</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>00120</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b</td>\n",
       "      <td>32.08</td>\n",
       "      <td>4.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>m</td>\n",
       "      <td>v</td>\n",
       "      <td>2.500</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00360</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b</td>\n",
       "      <td>33.17</td>\n",
       "      <td>1.040</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>r</td>\n",
       "      <td>h</td>\n",
       "      <td>6.500</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00164</td>\n",
       "      <td>31285</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>22.92</td>\n",
       "      <td>11.585</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>cc</td>\n",
       "      <td>v</td>\n",
       "      <td>0.040</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00080</td>\n",
       "      <td>1349</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b</td>\n",
       "      <td>54.42</td>\n",
       "      <td>0.500</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>h</td>\n",
       "      <td>3.960</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00180</td>\n",
       "      <td>314</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b</td>\n",
       "      <td>42.50</td>\n",
       "      <td>4.915</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.165</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00052</td>\n",
       "      <td>1442</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b</td>\n",
       "      <td>22.08</td>\n",
       "      <td>0.830</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>h</td>\n",
       "      <td>2.165</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00128</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b</td>\n",
       "      <td>29.92</td>\n",
       "      <td>1.835</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>h</td>\n",
       "      <td>4.335</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00260</td>\n",
       "      <td>200</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>a</td>\n",
       "      <td>38.25</td>\n",
       "      <td>6.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>1.000</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b</td>\n",
       "      <td>48.08</td>\n",
       "      <td>6.040</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>0.040</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>2690</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a</td>\n",
       "      <td>45.83</td>\n",
       "      <td>10.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>v</td>\n",
       "      <td>5.000</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>7</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b</td>\n",
       "      <td>36.67</td>\n",
       "      <td>4.415</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>0.250</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>10</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00320</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b</td>\n",
       "      <td>28.25</td>\n",
       "      <td>0.875</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>m</td>\n",
       "      <td>v</td>\n",
       "      <td>0.960</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00396</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>a</td>\n",
       "      <td>23.25</td>\n",
       "      <td>5.875</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>v</td>\n",
       "      <td>3.170</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>10</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00120</td>\n",
       "      <td>245</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>b</td>\n",
       "      <td>21.83</td>\n",
       "      <td>0.250</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>0.665</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a</td>\n",
       "      <td>19.17</td>\n",
       "      <td>8.585</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>cc</td>\n",
       "      <td>h</td>\n",
       "      <td>0.750</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>7</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00096</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>b</td>\n",
       "      <td>25.00</td>\n",
       "      <td>11.250</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>2.500</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>17</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00200</td>\n",
       "      <td>1208</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b</td>\n",
       "      <td>23.25</td>\n",
       "      <td>1.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>0.835</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>00300</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>a</td>\n",
       "      <td>47.75</td>\n",
       "      <td>8.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>7.875</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>1260</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a</td>\n",
       "      <td>27.42</td>\n",
       "      <td>14.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>x</td>\n",
       "      <td>h</td>\n",
       "      <td>3.085</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00120</td>\n",
       "      <td>11</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>a</td>\n",
       "      <td>41.17</td>\n",
       "      <td>6.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>v</td>\n",
       "      <td>0.500</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00145</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a</td>\n",
       "      <td>15.83</td>\n",
       "      <td>0.585</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>h</td>\n",
       "      <td>1.500</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00100</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>a</td>\n",
       "      <td>47.00</td>\n",
       "      <td>13.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>i</td>\n",
       "      <td>bb</td>\n",
       "      <td>5.165</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>9</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>b</td>\n",
       "      <td>56.58</td>\n",
       "      <td>18.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>d</td>\n",
       "      <td>bb</td>\n",
       "      <td>15.000</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>17</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>b</td>\n",
       "      <td>57.42</td>\n",
       "      <td>8.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>e</td>\n",
       "      <td>h</td>\n",
       "      <td>7.000</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b</td>\n",
       "      <td>42.08</td>\n",
       "      <td>1.040</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>5.000</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00500</td>\n",
       "      <td>10000</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>b</td>\n",
       "      <td>22.25</td>\n",
       "      <td>9.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.085</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>b</td>\n",
       "      <td>29.83</td>\n",
       "      <td>3.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>0.165</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00216</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>a</td>\n",
       "      <td>23.50</td>\n",
       "      <td>1.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>0.875</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00160</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>b</td>\n",
       "      <td>32.08</td>\n",
       "      <td>4.000</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>cc</td>\n",
       "      <td>v</td>\n",
       "      <td>1.500</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00120</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>b</td>\n",
       "      <td>31.08</td>\n",
       "      <td>1.500</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>0.040</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>00160</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>b</td>\n",
       "      <td>31.83</td>\n",
       "      <td>0.040</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>m</td>\n",
       "      <td>v</td>\n",
       "      <td>0.040</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>a</td>\n",
       "      <td>21.75</td>\n",
       "      <td>11.750</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>0.250</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00180</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>a</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>1.750</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00080</td>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>b</td>\n",
       "      <td>30.33</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>0.085</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>s</td>\n",
       "      <td>00252</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>b</td>\n",
       "      <td>51.83</td>\n",
       "      <td>2.040</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>1.500</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00120</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>b</td>\n",
       "      <td>47.17</td>\n",
       "      <td>5.835</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>5.500</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00465</td>\n",
       "      <td>150</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>b</td>\n",
       "      <td>25.83</td>\n",
       "      <td>12.835</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>cc</td>\n",
       "      <td>v</td>\n",
       "      <td>0.500</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>a</td>\n",
       "      <td>50.25</td>\n",
       "      <td>0.835</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.500</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00240</td>\n",
       "      <td>117</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.50</td>\n",
       "      <td>2.000</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>e</td>\n",
       "      <td>h</td>\n",
       "      <td>2.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00256</td>\n",
       "      <td>17</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>a</td>\n",
       "      <td>37.33</td>\n",
       "      <td>2.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>i</td>\n",
       "      <td>h</td>\n",
       "      <td>0.210</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00260</td>\n",
       "      <td>246</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>a</td>\n",
       "      <td>41.58</td>\n",
       "      <td>1.040</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.665</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00240</td>\n",
       "      <td>237</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>a</td>\n",
       "      <td>30.58</td>\n",
       "      <td>10.665</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>0.085</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>12</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00129</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>b</td>\n",
       "      <td>19.42</td>\n",
       "      <td>7.250</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>m</td>\n",
       "      <td>v</td>\n",
       "      <td>0.040</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00100</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>a</td>\n",
       "      <td>17.92</td>\n",
       "      <td>10.210</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>0.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>50</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>a</td>\n",
       "      <td>20.08</td>\n",
       "      <td>1.250</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>0.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>b</td>\n",
       "      <td>19.50</td>\n",
       "      <td>0.290</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>k</td>\n",
       "      <td>v</td>\n",
       "      <td>0.290</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>364</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.000</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>d</td>\n",
       "      <td>h</td>\n",
       "      <td>3.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00176</td>\n",
       "      <td>537</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>b</td>\n",
       "      <td>17.08</td>\n",
       "      <td>3.290</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>i</td>\n",
       "      <td>v</td>\n",
       "      <td>0.335</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00140</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>b</td>\n",
       "      <td>36.42</td>\n",
       "      <td>0.750</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>d</td>\n",
       "      <td>v</td>\n",
       "      <td>0.585</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00240</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>b</td>\n",
       "      <td>40.58</td>\n",
       "      <td>3.290</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>m</td>\n",
       "      <td>v</td>\n",
       "      <td>3.500</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>s</td>\n",
       "      <td>00400</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>b</td>\n",
       "      <td>21.08</td>\n",
       "      <td>10.085</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>e</td>\n",
       "      <td>h</td>\n",
       "      <td>1.250</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00260</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>a</td>\n",
       "      <td>22.67</td>\n",
       "      <td>0.750</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>2.000</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00200</td>\n",
       "      <td>394</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>a</td>\n",
       "      <td>25.25</td>\n",
       "      <td>13.500</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>2.000</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00200</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>b</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.205</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.040</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>750</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>b</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3.375</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>h</td>\n",
       "      <td>8.290</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2  3  4   5   6       7  8  9   10 11 12     13     14 15\n",
       "0      b  30.83   0.000  u  g   w   v   1.250  t  t   1  f  g  00202      0  +\n",
       "1      a  58.67   4.460  u  g   q   h   3.040  t  t   6  f  g  00043    560  +\n",
       "2      a  24.50   0.500  u  g   q   h   1.500  t  f   0  f  g  00280    824  +\n",
       "3      b  27.83   1.540  u  g   w   v   3.750  t  t   5  t  g  00100      3  +\n",
       "4      b  20.17   5.625  u  g   w   v   1.710  t  f   0  f  s  00120      0  +\n",
       "5      b  32.08   4.000  u  g   m   v   2.500  t  f   0  t  g  00360      0  +\n",
       "6      b  33.17   1.040  u  g   r   h   6.500  t  f   0  t  g  00164  31285  +\n",
       "7      a  22.92  11.585  u  g  cc   v   0.040  t  f   0  f  g  00080   1349  +\n",
       "8      b  54.42   0.500  y  p   k   h   3.960  t  f   0  f  g  00180    314  +\n",
       "9      b  42.50   4.915  y  p   w   v   3.165  t  f   0  t  g  00052   1442  +\n",
       "10     b  22.08   0.830  u  g   c   h   2.165  f  f   0  t  g  00128      0  +\n",
       "11     b  29.92   1.835  u  g   c   h   4.335  t  f   0  f  g  00260    200  +\n",
       "12     a  38.25   6.000  u  g   k   v   1.000  t  f   0  t  g  00000      0  +\n",
       "13     b  48.08   6.040  u  g   k   v   0.040  f  f   0  f  g  00000   2690  +\n",
       "14     a  45.83  10.500  u  g   q   v   5.000  t  t   7  t  g  00000      0  +\n",
       "15     b  36.67   4.415  y  p   k   v   0.250  t  t  10  t  g  00320      0  +\n",
       "16     b  28.25   0.875  u  g   m   v   0.960  t  t   3  t  g  00396      0  +\n",
       "17     a  23.25   5.875  u  g   q   v   3.170  t  t  10  f  g  00120    245  +\n",
       "18     b  21.83   0.250  u  g   d   h   0.665  t  f   0  t  g  00000      0  +\n",
       "19     a  19.17   8.585  u  g  cc   h   0.750  t  t   7  f  g  00096      0  +\n",
       "20     b  25.00  11.250  u  g   c   v   2.500  t  t  17  f  g  00200   1208  +\n",
       "21     b  23.25   1.000  u  g   c   v   0.835  t  f   0  f  s  00300      0  +\n",
       "22     a  47.75   8.000  u  g   c   v   7.875  t  t   6  t  g  00000   1260  +\n",
       "23     a  27.42  14.500  u  g   x   h   3.085  t  t   1  f  g  00120     11  +\n",
       "24     a  41.17   6.500  u  g   q   v   0.500  t  t   3  t  g  00145      0  +\n",
       "25     a  15.83   0.585  u  g   c   h   1.500  t  t   2  f  g  00100      0  +\n",
       "26     a  47.00  13.000  u  g   i  bb   5.165  t  t   9  t  g  00000      0  +\n",
       "27     b  56.58  18.500  u  g   d  bb  15.000  t  t  17  t  g  00000      0  +\n",
       "28     b  57.42   8.500  u  g   e   h   7.000  t  t   3  f  g  00000      0  +\n",
       "29     b  42.08   1.040  u  g   w   v   5.000  t  t   6  t  g  00500  10000  +\n",
       "..   ...    ...     ... .. ..  ..  ..     ... .. ..  .. .. ..    ...    ... ..\n",
       "660    b  22.25   9.000  u  g  aa   v   0.085  f  f   0  f  g  00000      0  -\n",
       "661    b  29.83   3.500  u  g   c   v   0.165  f  f   0  f  g  00216      0  -\n",
       "662    a  23.50   1.500  u  g   w   v   0.875  f  f   0  t  g  00160      0  -\n",
       "663    b  32.08   4.000  y  p  cc   v   1.500  f  f   0  t  g  00120      0  -\n",
       "664    b  31.08   1.500  y  p   w   v   0.040  f  f   0  f  s  00160      0  -\n",
       "665    b  31.83   0.040  y  p   m   v   0.040  f  f   0  f  g  00000      0  -\n",
       "666    a  21.75  11.750  u  g   c   v   0.250  f  f   0  t  g  00180      0  -\n",
       "667    a  17.92   0.540  u  g   c   v   1.750  f  t   1  t  g  00080      5  -\n",
       "668    b  30.33   0.500  u  g   d   h   0.085  f  f   0  t  s  00252      0  -\n",
       "669    b  51.83   2.040  y  p  ff  ff   1.500  f  f   0  f  g  00120      1  -\n",
       "670    b  47.17   5.835  u  g   w   v   5.500  f  f   0  f  g  00465    150  -\n",
       "671    b  25.83  12.835  u  g  cc   v   0.500  f  f   0  f  g  00000      2  -\n",
       "672    a  50.25   0.835  u  g  aa   v   0.500  f  f   0  t  g  00240    117  -\n",
       "673  NaN  29.50   2.000  y  p   e   h   2.000  f  f   0  f  g  00256     17  -\n",
       "674    a  37.33   2.500  u  g   i   h   0.210  f  f   0  f  g  00260    246  -\n",
       "675    a  41.58   1.040  u  g  aa   v   0.665  f  f   0  f  g  00240    237  -\n",
       "676    a  30.58  10.665  u  g   q   h   0.085  f  t  12  t  g  00129      3  -\n",
       "677    b  19.42   7.250  u  g   m   v   0.040  f  t   1  f  g  00100      1  -\n",
       "678    a  17.92  10.210  u  g  ff  ff   0.000  f  f   0  f  g  00000     50  -\n",
       "679    a  20.08   1.250  u  g   c   v   0.000  f  f   0  f  g  00000      0  -\n",
       "680    b  19.50   0.290  u  g   k   v   0.290  f  f   0  f  g  00280    364  -\n",
       "681    b  27.83   1.000  y  p   d   h   3.000  f  f   0  f  g  00176    537  -\n",
       "682    b  17.08   3.290  u  g   i   v   0.335  f  f   0  t  g  00140      2  -\n",
       "683    b  36.42   0.750  y  p   d   v   0.585  f  f   0  f  g  00240      3  -\n",
       "684    b  40.58   3.290  u  g   m   v   3.500  f  f   0  t  s  00400      0  -\n",
       "685    b  21.08  10.085  y  p   e   h   1.250  f  f   0  f  g  00260      0  -\n",
       "686    a  22.67   0.750  u  g   c   v   2.000  f  t   2  t  g  00200    394  -\n",
       "687    a  25.25  13.500  y  p  ff  ff   2.000  f  t   1  t  g  00200      1  -\n",
       "688    b  17.92   0.205  u  g  aa   v   0.040  f  f   0  f  g  00280    750  -\n",
       "689    b  35.00   3.375  u  g   c   h   8.290  f  f   0  t  g  00000      0  -\n",
       "\n",
       "[690 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "base = pd.read_csv('./credit.data', header=None, sep=',')\n",
    "base = base.replace('?', np.nan)\n",
    "data = base.values\n",
    "X, Y = data[:,:-1], data[:, -1]\n",
    "\n",
    "display(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 666 Lists of Patches objects>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "nbase = base[[1,2,7,10,13,14,15]]\n",
    "nbase = nbase.dropna()\n",
    "nbase = nbase.values\n",
    "X_n,Y_n=nbase[:,:-1],nbase[:,-1]\n",
    "X_n= X_n.astype(float)\n",
    "X_n.shape\n",
    "\n",
    "Y_n[Y_n == '+'] = 1\n",
    "Y_n[Y_n == '-'] = 0\n",
    "\n",
    "plt.hist(Y_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données positifs et négatifs sont répartis de la même façon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(666,)\n",
      "(666, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([0, 1], dtype=object),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-09a29e5d85fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrun_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-9902e84d12b6>\u001b[0m in \u001b[0;36mrun_classifiers\u001b[0;34m(clfs, X, Y)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0minit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mcv_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mkappa_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkappa_scorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 209\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0;31m# This is the first call to partial_fit:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;31m# initialize various cumulative counters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36m_check_partial_fit_first_call\u001b[0;34m(clf, classes)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# This is the first call to partial_fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([0, 1], dtype=object),)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_c = StandardScaler().fit_transform(X_n)\n",
    "print(Y_n.shape)\n",
    "print(X_c.shape)\n",
    "run_classifiers(clfs, X_c, Y_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut remarquer de faibles valeurs pour les mesures AUC et accuracy par rapport aux autres jeu de données. Cela peut être expliqué par un très grand nombre d'instance et peu d'attributs ce qui peut induire à un manque de relation pertinentes entres les attributs entrants et les sorties outputs .\n",
    "\n",
    "On remarque aussi que la méthode MLP ( MultilayerPerceptron ) prend considérablement plus de temps que les autres modèles prédictifs vue le grand nombre d'itérations executées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32303239 0.17812337 0.1624939  0.13241474]\n",
      "[ 0.45499012  0.47793667  0.53532047  0.45612324 -0.23760746  0.11613271]\n",
      "[ 0.07582484 -0.06039093  0.12243534  0.01486247  0.67874305  0.71742602]\n",
      "[ 0.07582484 -0.06039093  0.12243534  0.01486247  0.67874305  0.71742602]\n",
      "[ 0.07582484 -0.06039093  0.12243534  0.01486247  0.67874305  0.71742602]\n",
      "-----------------------\n",
      "[[-0.06204687 -0.9593182  -0.29067118 -0.2962713   0.11604602 -0.19206984]\n",
      " [ 2.27524482 -0.06759466  0.24444227  0.71873224 -0.81188397 -0.08435822]\n",
      " [-0.59347849 -0.85934919 -0.21593467 -0.49927201  0.57125695 -0.03357988]\n",
      " ...\n",
      " [-0.53051266  1.73984498 -0.06646163 -0.2962713   0.10437394 -0.1918775 ]\n",
      " [-1.14589873 -0.9183309  -0.65239591 -0.49927201  0.57125695 -0.0478132 ]\n",
      " [ 0.28804316 -0.2845274   1.81390909 -0.49927201 -1.06283359 -0.19206984]]\n",
      "-----------------------\n",
      "[[-0.82734151 -0.0457928  -0.41501095 -0.08927876]\n",
      " [ 1.64470632 -0.39436869 -0.76993872 -0.84593745]\n",
      " [-1.16369979  0.33668363 -0.29212106  0.12123701]\n",
      " ...\n",
      " [ 0.37235991 -0.22465229  0.73624259  0.07782321]\n",
      " [-1.67853466  0.23470878  0.06806176  0.41929593]\n",
      " [ 0.9685954  -0.60549695 -0.45399    -0.80904955]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.75)\n",
    "pca.fit(X_c)\n",
    "X_t = pca.transform(X_c)\n",
    "print(pca.explained_variance_ratio_)\n",
    "pca_1 = pca.components_[0]\n",
    "pca_2 = pca.components_[1]\n",
    "pca_3 = pca.components_[1]\n",
    "pca_4 = pca.components_[1]\n",
    "print(pca_1)\n",
    "print(pca_2)\n",
    "print(pca_3)\n",
    "print(pca_4)\n",
    "print('-----------------------')\n",
    "print(X_c)\n",
    "print('-----------------------')\n",
    "print(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([0, 1], dtype=object),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4473b8efc6e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_concat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manswer_data_concat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-b7ecc352d79a>\u001b[0m in \u001b[0;36mrun_classifiers\u001b[0;34m(clfs, X, Y)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0minit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mcv_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mkappa_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkappa_scorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 209\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0;31m# This is the first call to partial_fit:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;31m# initialize various cumulative counters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36m_check_partial_fit_first_call\u001b[0;34m(clf, classes)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# This is the first call to partial_fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([0, 1], dtype=object),)"
     ]
    }
   ],
   "source": [
    "data_concat=np.concatenate((X_c,X_t),axis=1)\n",
    "answer_data_concat=run_classifiers(clfs, data_concat, Y_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Imputer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e7f5e04a5040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Imputer pour des variables catégoriques :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mX_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Imputer'"
     ]
    }
   ],
   "source": [
    "base = pd.read_csv('./credit.data', header=None, sep=',')\n",
    "data=base.values\n",
    "X, Y= data[:, :-1], data[:, -1]\n",
    "\n",
    "base_n = base[[1,2,7,10,13,14,15]]\n",
    "base_n = base_n.values\n",
    "X_n, Y_n = base_n[:, :-1], base_n[:, -1]\n",
    "\n",
    "base_c = base[[0,3,4,5,6,8,9,11,12]]\n",
    "base_c = base_c.values\n",
    "\n",
    "#Imputer pour des variables catégoriques :\n",
    "from sklearn.preprocessing import Imputer\n",
    "X_cat = np.copy(base_c)\n",
    "for col_id in range(0,9):\n",
    "    unique_val, val_idx = np.unique(X_cat[:, col_id], return_inverse=True)\n",
    "    X_cat[:, col_id] = val_idx\n",
    "imp_cat = Imputer(missing_values=0, strategy='most_frequent')\n",
    "X_cat[:, range(5)] = imp_cat.fit_transform(X_cat[:, range(5)])\n",
    "\n",
    "\n",
    "#Imputer pour des variables numériques :\n",
    "X_num = np.copy(X_n)\n",
    "X_num[X_num == '?'] = np.nan\n",
    "X_num = X_num.astype(float)\n",
    "imp_num = Imputer(missing_values=np.nan, strategy='mean')\n",
    "X_num = imp_num.fit_transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6bb880e983d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_cat_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cat_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Ajout des donnée des composantes PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_cat' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X_cat_bin = OneHotEncoder().fit_transform(X_cat).toarray()\n",
    "X_num = StandardScaler().fit_transform(X_num)\n",
    "s = np.concatenate((X_num, X_cat_bin), axis=1)\n",
    "# Ajout des donnée des composantes PCA\n",
    "pca=PCA(n_components=0.75)\n",
    "s_pca=pca.fit_transform(s)\n",
    "s_total=np.append(s,s_pca,axis=1)\n",
    "run_classifiers(clfs,s_total,np.where(Y_n==\"+\",1,0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for NBayes is: 0.734 +/- 0.048\n",
    "\n",
    "kappa for NBayes is: 0.706 +/- 0.062\n",
    "\n",
    "AUC for NBayes is: 0.429 +/- 0.118\n",
    "\n",
    "execution time is: 0.06654763221740723 seconds\n",
    "\n",
    "\n",
    "Accuracy for CART is: 0.801 +/- 0.047\n",
    "\n",
    "kappa for CART is: 0.803 +/- 0.065\n",
    "\n",
    "AUC for CART is: 0.572 +/- 0.128\n",
    "\n",
    "execution time is: 0.24431705474853516 seconds\n",
    "\n",
    "\n",
    "\n",
    "Accuracy for ID3 is: 0.807 +/- 0.034\n",
    "\n",
    "kappa for ID3 is: 0.802 +/- 0.050\n",
    "\n",
    "AUC for ID3 is: 0.595 +/- 0.093\n",
    "\n",
    "execution time is: 0.3808708190917969 seconds\n",
    "\n",
    "\n",
    "Accuracy for MLP is: 0.863 +/- 0.045\n",
    "\n",
    "kappa for MLP is: 0.855 +/- 0.055\n",
    "\n",
    "AUC for MLP is: 0.714 +/- 0.101\n",
    "\n",
    "execution time is: 12.614310264587402 seconds\n",
    "\n",
    "\n",
    "\n",
    "Accuracy for RF is: 0.863 +/- 0.043\n",
    "\n",
    "kappa for RF is: 0.862 +/- 0.037\n",
    "\n",
    "AUC for RF is: 0.713 +/- 0.054\n",
    "\n",
    "execution time is: 3.1260945796966553 seconds\n",
    "\n",
    "\n",
    "Accuracy for KNN is: 0.834 +/- 0.046\n",
    "\n",
    "kappa for KNN is: 0.827 +/- 0.046\n",
    "\n",
    "AUC for KNN is: 0.659 +/- 0.092\n",
    "\n",
    "execution time is: 0.1940932273864746 seconds\n",
    "\n",
    "\n",
    "Accuracy for Bagging is: 0.856 +/- 0.044\n",
    "\n",
    "kappa for Bagging is: 0.866 +/- 0.040\n",
    "\n",
    "AUC for Bagging is: 0.714 +/- 0.086\n",
    "\n",
    "execution time is: 3.13608980178833 seconds\n",
    "\n",
    "\n",
    "Accuracy for Adaboost_depth1 is: 0.828 +/- 0.041\n",
    "\n",
    "kappa for Adaboost_depth1 is: 0.828 +/- 0.040\n",
    "\n",
    "AUC for Adaboost_depth1 is: 0.652 +/- 0.083\n",
    "\n",
    "execution time is: 3.9681475162506104 seconds\n",
    "\n",
    "\n",
    "Accuracy for Adaboost_depth2 is: 0.823 +/- 0.047\n",
    "\n",
    "kappa for Adaboost_depth2 is: 0.823 +/- 0.047\n",
    "\n",
    "AUC for Adaboost_depth2 is: 0.643 +/- 0.091\n",
    "\n",
    "execution time is: 5.40229868888855 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque une nette amélioration dans les mesures cela peut être expliqué par la diminution des valeurs manquantes et la normalisation de notre ensemble de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Apprentissage supervisé sur des données textuelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for NBayes is: 0.901 +/- 0.015\n",
      "kappa for NBayes is: 0.906 +/- 0.015\n",
      "AUC for NBayes is: 0.655 +/- 0.042\n",
      "execution time is: 36.69809579849243 seconds\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-77821d4d2ae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Premier test en essayant d'entrainer plusieurs classificateurs sur la matrice complète (300 Colonnes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mrun_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspam_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-082aaf28a16f>\u001b[0m in \u001b[0;36mrun_classifiers\u001b[0;34m(clfs, X, Y)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcv_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mkappa_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauc_scorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mauc_scores\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkappa_scorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Chargement des données textuelles\n",
    "text_data = pd.read_csv(\"SMSSpamCollection\", sep='\\t')\n",
    "# Séparation des données en document / prediction\n",
    "document_class  = text_data.iloc[:,0].values\n",
    "document_text   = text_data.iloc[:,1].values\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# Transformation des données textuelles en Bag of words (Tokens avec leur fréquence)\n",
    "X = vectorizer.fit_transform(document_text)\n",
    "# Test de la transformation appliquée sur une phrase ( Comment l'algorithme sépare les mots et est ce qu'il élimine les caractères spéciaux)\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\")\n",
    "\n",
    "# Afficher tout les mots uniques existant\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "# Transformation de chaque ligne en donnée d'une matrice ou chaque case représente la fraquence du mot dans la colonne ( Les colonnes représentent tout les mots existants)\n",
    "spam_train = X.toarray()\n",
    "# Binarisation des classes : Spam => 1 et Ham => 0\n",
    "a          = pd.factorize(document_class)[0]\n",
    "\n",
    "# Premier test en essayant d'entrainer plusieurs classificateurs sur la matrice complète (300 Colonnes)\n",
    "run_classifiers(clfs, spam_train, a)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que nous obtenons des résultat intéréssants pour plusieurs classificateurs, mais le temps pour apprendre est très grand en le comparant aux exemples faits dans la 1ère et la deuxième partie.\n",
    "\n",
    "Ceci est du au fait que nous travaillons sur des inputs en très grande dimension ( 300 colonnes ) et pour certains classficateurs nous remarquons que des performances tel que l'AUC est si proche de 0.5, ce qui veut dire qu'il est nécessaire encore d'autres étapes de preprocessing et de feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf–idf term weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cette partie est de pondérer la fréquence de quelques termes par un coefficient qui symbolise l'importance de sa prise \n",
    "en compte dans la décision de classification. En effet les classificateur sont indifférent au mot, et ne font que calculer leur fréquences.\n",
    "Certes il y'a des mots ( Comme les pronoms relatives, pronoms possessifs) qui peuvent être présents dans plusieurs textes,\n",
    "et leur importance dans la matrice des fréquences doit être affaiblie, sinon ils vont empêcher des mots clés important d'être pris en compte et notre classification va être biaisée.\n",
    "\n",
    "C'est ainsi qu'intervientle rôle du TF-idf weighting qui fonctionne comme suit : \n",
    "\n",
    "Soit d le message , et t le terme et n le total du nombre des documents.\n",
    "\n",
    "La Nouvelle fréquence du terme t dans le sms d $Tf-id(t,d)$ est égalé à la l'ancienne fréquence $tf(t,d)$ multipliée fois un facteur $idf(t)$ qui représente l'importance du mot.\n",
    "\n",
    "\n",
    "$Tf-id(t,d) = tf(t,d) *idf(t) = (log \\frac{1+n}{1+df(t)} + 1 ) * idf(t) ( avec df(t) le nombre de documents qui contiennent le terme t$\n",
    "\n",
    "Ainsi si par exemple un mot est présent dans tout les documents => $log \\frac{1+n}{1+df(t)} + 1 = log \\frac{1+n}{1+n)} + 1 = 1$\n",
    "\n",
    "L'importance est alors égale à 1\n",
    "\n",
    "Autrement si un mot n'est présent que dans 30% des documents son importance est augmentée =>\n",
    "\n",
    "$log \\frac{1+n}{1+df(t)} + 1 = log \\frac{1+n}{1+0.3*n} + 1 > 1$\n",
    "\n",
    "Et finalement les colonnes sont normalisés.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "transformer = TfidfVectorizer()\n",
    "\n",
    "X2 = transformer.fit_transform(document_text)\n",
    "spam_train2 = X2.toarray()\n",
    "\n",
    "#run_classifiers(clfs, spam_train2, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Le but de cette partie est de réduire la dimension des colonnes en sélectionnant par exemple les 10 colonnes qui expliquent\n",
    "le mieux notre variance, nous procédons alors comme suit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.94808714e-02  1.03746698e-01  2.35648693e-01 ... -6.58718965e-04\n",
      "  -1.23806706e-03 -5.47718662e-04]\n",
      " [ 9.29722153e-02  2.19027584e-03 -6.03888567e-04 ...  1.08715585e-01\n",
      "   7.75557202e-02  5.10183499e-03]\n",
      " [ 5.70926520e-02  1.51318928e-02  5.74833473e-02 ... -9.12559396e-03\n",
      "   2.00080371e-03 -2.66370763e-02]\n",
      " ...\n",
      " [ 1.15028357e-01 -2.58683622e-03  7.99502660e-03 ... -4.30981865e-02\n",
      "  -8.32289815e-03 -2.19110589e-02]\n",
      " [ 1.99630144e-01 -1.00747410e-02  2.73454496e-02 ...  5.33701187e-02\n",
      "   2.15134527e-02  3.31124528e-02]\n",
      " [ 7.95791081e-02 -1.27989020e-02  2.16332670e-02 ...  7.74831236e-02\n",
      "  -2.07448149e-05 -2.44496024e-02]]\n",
      "Accuracy for NBayes is: 0.946 +/- 0.012\n",
      "kappa for NBayes is: 0.922 +/- 0.013\n",
      "AUC for NBayes is: 0.783 +/- 0.038\n",
      "execution time is: 0.0671238899230957 seconds\n",
      "\n",
      "\n",
      "Accuracy for CART is: 0.950 +/- 0.008\n",
      "kappa for CART is: 0.907 +/- 0.011\n",
      "AUC for CART is: 0.797 +/- 0.022\n",
      "execution time is: 1.2410485744476318 seconds\n",
      "\n",
      "\n",
      "Accuracy for ID3 is: 0.956 +/- 0.008\n",
      "kappa for ID3 is: 0.913 +/- 0.014\n",
      "AUC for ID3 is: 0.814 +/- 0.031\n",
      "execution time is: 1.5337376594543457 seconds\n",
      "\n",
      "\n",
      "Accuracy for MLP is: 0.962 +/- 0.006\n",
      "kappa for MLP is: 0.891 +/- 0.021\n",
      "AUC for MLP is: 0.814 +/- 0.037\n",
      "execution time is: 35.14028239250183 seconds\n",
      "\n",
      "\n",
      "Accuracy for RF is: 0.971 +/- 0.008\n",
      "kappa for RF is: 0.911 +/- 0.019\n",
      "AUC for RF is: 0.863 +/- 0.028\n",
      "execution time is: 11.250213861465454 seconds\n",
      "\n",
      "\n",
      "Accuracy for KNN is: 0.963 +/- 0.007\n",
      "kappa for KNN is: 0.914 +/- 0.013\n",
      "AUC for KNN is: 0.840 +/- 0.024\n",
      "execution time is: 0.9618015289306641 seconds\n",
      "\n",
      "\n",
      "Accuracy for Bagging is: 0.926 +/- 0.010\n",
      "kappa for Bagging is: 0.851 +/- 0.026\n",
      "AUC for Bagging is: 0.689 +/- 0.038\n",
      "execution time is: 6.708030700683594 seconds\n",
      "\n",
      "\n",
      "Accuracy for Adaboost_depth1 is: 0.962 +/- 0.009\n",
      "kappa for Adaboost_depth1 is: 0.910 +/- 0.016\n",
      "AUC for Adaboost_depth1 is: 0.833 +/- 0.031\n",
      "execution time is: 9.973195314407349 seconds\n",
      "\n",
      "\n",
      "Accuracy for Adaboost_depth2 is: 0.965 +/- 0.009\n",
      "kappa for Adaboost_depth2 is: 0.913 +/- 0.018\n",
      "AUC for Adaboost_depth2 is: 0.847 +/- 0.037\n",
      "execution time is: 16.458856105804443 seconds\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NBayes': GaussianNB(priors=None),\n",
       " 'CART': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       " 'ID3': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       " 'MLP': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "        beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "        hidden_layer_sizes=(20, 10), learning_rate='constant',\n",
       "        learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "        nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "        shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "        verbose=False, warm_start=False),\n",
       " 'RF': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'KNN': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'),\n",
       " 'Bagging': BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "          bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "          max_samples=1.0, n_estimators=50, n_jobs=1, oob_score=False,\n",
       "          random_state=None, verbose=0, warm_start=False),\n",
       " 'Adaboost_depth1': AdaBoostClassifier(algorithm='SAMME.R',\n",
       "           base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       " 'Adaboost_depth2': AdaBoostClassifier(algorithm='SAMME.R',\n",
       "           base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=10, n_iter=7, random_state=42)\n",
    "spam_reduced = svd.fit_transform(spam_train2)\n",
    "print(spam_reduced)\n",
    "\n",
    "run_classifiers(clfs, spam_reduced, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cette partie est de rassembler toutes les parties précédentes dans un Pipeline.\n",
    "\n",
    "Nous procédons de la manière suivante :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pipeline = Pipeline(memory=None,steps=[\n",
    "    ('vect',TfidfVectorizer()),\n",
    "    ('reduce_dim', TruncatedSVD(n_components=10, n_iter=7, random_state=42)),\n",
    "    ('clf', DecisionTreeClassifier())] )\n",
    "\n",
    "# Apprentissage et test sur des nouvelles prédictions\n",
    "\n",
    "pipeline.fit(document_text,a).predict(['Free vacation', 'Come to work this morning','win gift card', 'your account is going to be suspended'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application pour le cas de Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  stars\n",
      "0  My wife took me here on my birthday for breakf...      5\n",
      "1  I have no idea why some people give bad review...      5\n",
      "2  love the gyro plate. Rice is so good and I als...      4\n",
      "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
      "4  General Manager Scott Petello is a good egg!!!...      5\n",
      "5  Quiessence is, simply put, beautiful.  Full wi...      4\n",
      "6  Drop what you're doing and drive here. After I...      5\n",
      "7  Luckily, I didn't have to travel far to make m...      4\n",
      "8  Definitely come for Happy hour! Prices are ama...      4\n",
      "9  Nobuo shows his unique talents with everything...      5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 3, 1, 5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv(\"yelp.csv\")\n",
    "useful_data = data[['text', 'stars']]\n",
    "# Affichage des 10 premières lignes\n",
    "print(useful_data.iloc[:10, :])\n",
    "\n",
    "yelp_text    = useful_data.iloc[:,0].values\n",
    "yelp_classes = useful_data.iloc[:,1].values\n",
    "\n",
    "# Test pour choisir le meilleur classificateur \n",
    "\"\"\"\n",
    "transformer    = TfidfVectorizer()\n",
    "yelp_X         = transformer.fit_transform(yelp_text).toarray()\n",
    "svd            = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n",
    "reduced_yelp_X = svd.fit_transform(yelp_X)\n",
    "run_classifiers(clfs,reduced_yelp_X,yelp_classes)\n",
    "\"\"\"\n",
    "\n",
    "pipeline = Pipeline(memory=None,steps=[\n",
    "    ('vect',TfidfVectorizer(stop_words=\"english\", max_features = 400)),\n",
    "    ('reduce_dim', TruncatedSVD(n_components=12, n_iter=7, random_state=42)),\n",
    "    ('clf', RandomForestClassifier())] )\n",
    "\n",
    "pipeline.fit(yelp_text,yelp_classes).predict([\"That was a great restauraunt\", \"I think the result was medium\", \"I didn't like the service\", \"Food was disgusting\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
